{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import (col, collect_list, concat,\n",
    "                                   create_map, lit, map_concat,\n",
    "                                   map_from_arrays, substring_index,\n",
    "                                   to_json, when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>generate-statistics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe2290ce7b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master('local[*]')\n",
    "         .config('spark.driver.memory', '10G')\n",
    "         .appName('generate-statistics')\n",
    "         .getOrCreate())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 500 Row Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 40s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "df_500 = spark.read.parquet('../data/df_500/')\n",
    "\n",
    "stats = ['count', 'mean', 'stddev', 'min', 'max',\n",
    "         '1%','5%', '25%', '50%', '75%', '95%', '99%']\n",
    "\n",
    "df_500_summary = (\n",
    "    df_500\n",
    "    .summary(*stats)\n",
    "    .withColumn('summary',\n",
    "                when(col('summary').contains('%'),\n",
    "                     concat(lit('p'),\n",
    "                            substring_index(col('summary'),\n",
    "                                            '%', 1)))\n",
    "                .otherwise(col('summary'))))\n",
    "\n",
    "df_500_summary_cols = df_500_summary.columns\n",
    "\n",
    "df_500_summary_metric_cols = [col for col in df_500_summary_cols if col != 'summary']\n",
    "\n",
    "df_500_metrics_long = spark.createDataFrame(\n",
    "    Row(name=col, metric=m_row['summary'], value=m_row[col])\n",
    "    for m_row in df_500_summary.toLocalIterator()\n",
    "    for col in df_500_summary_metric_cols)\n",
    "\n",
    "def update_metrics_map(row):\n",
    "    row['metrics_map'].update({'name': row['name']})\n",
    "    return row['metrics_map']\n",
    "\n",
    "result_list = (\n",
    "    df_500_metrics_long\n",
    "    .withColumn('value', col('value').astype('double'))\n",
    "    .groupBy(col('name'))\n",
    "    .agg(collect_list(col('metric')).alias('metric_array'),\n",
    "         collect_list('value').alias('value_array'))\n",
    "    .withColumn('metrics_map', map_from_arrays(col('metric_array'),\n",
    "                                               col('value_array')))\n",
    "    .select('name', 'metrics_map')\n",
    "    .orderBy(col('name'))\n",
    "    .rdd.map(lambda x: update_metrics_map(x))\n",
    "    .collect())\n",
    "\n",
    "with open('../data/metrics_for_500.json', 'w') as f:\n",
    "    json.dump(result_list, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the Distributed Stage in the Summary Task:** Input Size / Records\t53.2 MB / 25000\n",
    "\n",
    "**Timing Information for whole job:** 2min 40s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3000 Row Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1h 18min 21s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "df_3000 = spark.read.parquet('../data/df_3000/')\n",
    "\n",
    "stats = ['count', 'mean', 'stddev', 'min', 'max',\n",
    "         '1%','5%', '25%', '50%', '75%', '95%', '99%']\n",
    "\n",
    "df_3000_summary = (\n",
    "    df_3000\n",
    "    .repartition(48)\n",
    "    .summary(*stats)\n",
    "    .withColumn('summary',\n",
    "                when(col('summary').contains('%'),\n",
    "                     concat(lit('p'),\n",
    "                            substring_index(col('summary'),\n",
    "                                            '%', 1)))\n",
    "                .otherwise(col('summary'))))\n",
    "\n",
    "df_3000_summary_cols = df_3000_summary.columns\n",
    "\n",
    "df_3000_summary_metric_cols = [col for col in df_3000_summary_cols if col != 'summary']\n",
    "\n",
    "df_3000_metrics_long = spark.createDataFrame(\n",
    "    Row(name=col, metric=m_row['summary'], value=m_row[col])\n",
    "    for m_row in df_3000_summary.toLocalIterator()\n",
    "    for col in df_3000_summary_metric_cols)\n",
    "\n",
    "def update_metrics_map(row):\n",
    "    row['metrics_map'].update({'name': row['name']})\n",
    "    return row['metrics_map']\n",
    "\n",
    "result_list = (\n",
    "    df_3000_metrics_long\n",
    "    .withColumn('value', col('value').astype('double'))\n",
    "    .groupBy(col('name'))\n",
    "    .agg(collect_list(col('metric')).alias('metric_array'),\n",
    "         collect_list('value').alias('value_array'))\n",
    "    .withColumn('metrics_map', map_from_arrays(col('metric_array'),\n",
    "                                               col('value_array')))\n",
    "    .select('name', 'metrics_map')\n",
    "    .orderBy(col('name'))\n",
    "    .rdd.map(lambda x: update_metrics_map(x))\n",
    "    .collect())\n",
    "\n",
    "with open('../data/metrics_for_3000.json', 'w') as f:\n",
    "    json.dump(result_list, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the Distributed Stage in the Summary Task:** Input Size / Records\t18.0 MB / 4168\n",
    "\n",
    "**Timing Information for whole job:** 1h 18min 21s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 36s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "df_500_of_3000 = spark.read.parquet('../data/df_3000/')\n",
    "\n",
    "stats = ['count', 'mean', 'stddev', 'min', 'max',\n",
    "         '1%','5%', '25%', '50%', '75%', '95%', '99%']\n",
    "\n",
    "df_500_of_3000_summary = (\n",
    "    df_500_of_3000\n",
    "    .select(*[col for col in df_500_of_3000.columns if col.endswith('_1')])\n",
    "    .repartition(8)\n",
    "    .summary(*stats)\n",
    "    .withColumn('summary',\n",
    "                when(col('summary').contains('%'),\n",
    "                     concat(lit('p'),\n",
    "                            substring_index(col('summary'),\n",
    "                                            '%', 1)))\n",
    "                .otherwise(col('summary'))))\n",
    "\n",
    "df_500_of_3000_summary_cols = df_500_of_3000_summary.columns\n",
    "\n",
    "df_500_of_3000_summary_metric_cols = [col for col in df_500_of_3000_summary_cols if col != 'summary']\n",
    "\n",
    "df_500_of_3000_metrics_long = spark.createDataFrame(\n",
    "    Row(name=col, metric=m_row['summary'], value=m_row[col])\n",
    "    for m_row in df_500_of_3000_summary.toLocalIterator()\n",
    "    for col in df_500_of_3000_summary_metric_cols)\n",
    "\n",
    "def update_metrics_map(row):\n",
    "    row['metrics_map'].update({'name': row['name']})\n",
    "    return row['metrics_map']\n",
    "\n",
    "result_list = (\n",
    "    df_500_of_3000_metrics_long\n",
    "    .withColumn('value', col('value').astype('double'))\n",
    "    .groupBy(col('name'))\n",
    "    .agg(collect_list(col('metric')).alias('metric_array'),\n",
    "         collect_list('value').alias('value_array'))\n",
    "    .withColumn('metrics_map', map_from_arrays(col('metric_array'),\n",
    "                                               col('value_array')))\n",
    "    .select('name', 'metrics_map')\n",
    "    .orderBy(col('name'))\n",
    "    .rdd.map(lambda x: update_metrics_map(x))\n",
    "    .collect())\n",
    "\n",
    "with open('../data/metrics_for_500_of_3000.json', 'w') as f:\n",
    "    json.dump(result_list, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the Distributed Stage in the Summary Task:** Input Size / Records\t47.2 MB / 11182\n",
    "\n",
    "**Timing Information for whole job:** 2min 36s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In it's current implementation, summary() seems to be dependent partially on number of columns.\n",
    "\n",
    "In the 3000 row run, the runtime is 36 times that of the 500 row run. 6x increase in the runtime was contributed by 6x repartitioning I needed to do to fit the partition in my memory. Remaining 6x seems to have been contributed by the 6x increase in the number of rows.\n",
    "\n",
    "The choice of parquet files as storage aids in breaking out the columns is aiding in the 500_from_3000 run as there seems to be no appreciable performance degradation in picking out the 500 columns from the 3000 as would probably have been the case with non-columnar storage say for example csv files.\n",
    "\n",
    "But the summary method implementation is not able to benefit from the parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe itself can be transformed to a long format (i.e. *col_name*, *value*) before the summarizing step, this should make it possible to scale linearly w.r.t. number of columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
