{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "from random import choices, randint, random\n",
    "\n",
    "from pyspark.sql import Row, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>create_500_row_dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdf0f8cb710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master('local[*]')\n",
    "#          .config(\"spark.driver.memory\", \"4g\")\n",
    "         .appName('create_500_row_dataframe')\n",
    "         .getOrCreate())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These constants are used as caps for lower and upper bounds\n",
    "#  generated in get_random_column_bounds()\n",
    "OVERALL_MIN_INT = 0\n",
    "OVERALL_MAX_INT = 1000\n",
    "OVERALL_MIN_DOUBLE = 0.0\n",
    "OVERALL_MAX_DOUBLE = 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_column_bounds(datatype):\n",
    "    \"\"\"Generate Randomized Lower and Upper bounds given a datatype.\n",
    "    \n",
    "    Params:\n",
    "        datatype (string) - Can be either 'int' or 'double'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple constituting (<lower bound>, <upper bound>)\n",
    "        \n",
    "    Details:\n",
    "        The lower and upper bounds will be capped by the values\n",
    "         of static globals:\n",
    "        - OVERALL_MIN_INT & OVERALL_MAX_INT\n",
    "        - OVERALL_MIN_DOUBLE & OVERALL_MAX_DOUBLE\n",
    "        \n",
    "        To look like real-world data:\n",
    "        - 'int' bounds will randomly look like either binary data\n",
    "            or positive integers\n",
    "        - 'double' bounds will randomly look like either normalized\n",
    "            or original valued\n",
    "    \"\"\"\n",
    "    if datatype == 'int':\n",
    "        if choices(['binary','discrete']) == ['binary']:\n",
    "            lower_bound = 0\n",
    "            upper_bound = 1\n",
    "        else:\n",
    "            lower_bound = randint(OVERALL_MIN_INT, int(OVERALL_MAX_INT*0.5))\n",
    "            upper_bound = randint(lower_bound, OVERALL_MAX_INT)\n",
    "    elif datatype == 'double':\n",
    "        if choices(['scaled', 'real']) == ['scaled']:\n",
    "            lower_bound = -1.0\n",
    "            upper_bound = 1.0\n",
    "        else:\n",
    "            lower_bound = (OVERALL_MIN_DOUBLE +\n",
    "                           (random() *\n",
    "                            ((OVERALL_MAX_DOUBLE * 0.5) -\n",
    "                             OVERALL_MIN_DOUBLE)))\n",
    "            upper_bound = (lower_bound +\n",
    "                           (random() *\n",
    "                            (OVERALL_MAX_DOUBLE - lower_bound)))\n",
    "    else:\n",
    "        raise AttributeError(\n",
    "            \"get_random_column_bounds() not defined for datatype: {}\"\n",
    "            .format(datatype))\n",
    "        \n",
    "    return (lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns(number_of_columns):\n",
    "    \"\"\"Create dictionary of column names and datatypes.\n",
    "    \n",
    "    Params:\n",
    "        number_of_columns (int) - Number of Columns to generate\n",
    "    \n",
    "    Returns:\n",
    "        {'<Column Name>': {'dtype': 'int'|'double',\n",
    "                           'bounds': (<lower_bound>, <upper_bound>),\n",
    "                           }\n",
    "\n",
    "    Details:\n",
    "    There is a equal probability in a column being marked\n",
    "     either an int or a double.\n",
    "    \"\"\"\n",
    "    number_of_digits = int(log10(number_of_columns)) + 1\n",
    "    column_name_format = \"col_{{:0{}d}}\".format(number_of_digits)\n",
    "    return {\n",
    "        column_name_format.format(i):\n",
    "        {'dtype': d,\n",
    "         'bounds': get_random_column_bounds(d)}\n",
    "        for i, d in enumerate(\n",
    "            choices(population=['int', 'double'],\n",
    "                    k=number_of_columns))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_observation(column_metadata):\n",
    "    \"\"\"Create a Random Observation given Column Metadata.\n",
    "    \n",
    "    Params:\n",
    "        column_metadata (dict):\n",
    "            Any of the values from dictionary returned by\n",
    "             create_columns\n",
    "    \n",
    "    Returns:\n",
    "        A random value based on datatype and bounds defined\n",
    "         in column metadata\n",
    "    \"\"\"\n",
    "    if column_metadata['dtype'] == 'int':\n",
    "        obs = randint(*column_metadata['bounds'])\n",
    "    elif column_metadata['dtype'] == 'double':\n",
    "        obs = (column_metadata['bounds'][0] +\n",
    "               (random() *\n",
    "                (column_metadata['bounds'][1] -\n",
    "                 column_metadata['bounds'][0])))\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row_dict(columns):\n",
    "    \"\"\"Creates a dictionary representing a Row given columns information.\n",
    "    \n",
    "    Params:\n",
    "        columns (dict):\n",
    "            Dictionary as generated by create_columns()\n",
    "            \n",
    "    Returns:\n",
    "        Dictionary with column names as keys and random values generated\n",
    "         by create_random_observation()\n",
    "    \"\"\"\n",
    "    return {\n",
    "        col_name: create_random_observation(col_metadata)\n",
    "        for col_name, col_metadata in columns.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(columns, num_rows):\n",
    "    \"\"\"Create DataFrame for columns with num_rows.\n",
    "    \n",
    "    Params:\n",
    "        - columns (dict):\n",
    "            Dictionary as generated by create_columns()\n",
    "        - num_rows (int):\n",
    "            Number of rows to create the Dataframe for\n",
    "    \n",
    "    Returns:\n",
    "        Spark Dataframe\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.sparkContext\n",
    "        .parallelize(range(num_rows))\n",
    "        .map(lambda x: Row(**create_row_dict(columns)))\n",
    "        .toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_500 = create_columns(500)\n",
    "\n",
    "# df_500 = create_dataframe(columns_500, 200000)\n",
    "# df_500.write.parquet('../data/df_500', mode='overwrite')\n",
    "\n",
    "############################################################\n",
    "# If running on a system where spinning up a 4G java       #\n",
    "# process is not an option, the dataset creation can be    #\n",
    "# batched using the following by tweaking num_batches      #\n",
    "############################################################\n",
    "def batch_create(path, columns, num_rows, num_batches):\n",
    "    (batch_size,\n",
    "     size_of_last_batch) = divmod(num_rows, num_batches)\n",
    "    \n",
    "    number_of_complete_batches = (num_batches\n",
    "                                  if size_of_last_batch == 0\n",
    "                                  else num_batches - 1)\n",
    "    \n",
    "    for batch in range(number_of_complete_batches):\n",
    "        df_500 = create_dataframe(columns_500, batch_size)\n",
    "        iter_mode = 'overwrite' if batch == 0 else 'append'\n",
    "        df_500.write.parquet(path, mode=iter_mode)\n",
    "        \n",
    "    if size_of_last_batch > 0:\n",
    "        df_500 = create_dataframe(columns_500, size_of_last_batch)\n",
    "        df_500.write.parquet(path, mode='append')\n",
    "        \n",
    "batch_create('../data/df_500', columns_500, 200000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"../data/df_500/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spark.read.parquet(\"../data/df_500/\").columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(\"../data/df_500/\").limit(50).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
