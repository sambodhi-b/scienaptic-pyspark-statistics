{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import json\n",
    "from pyspark.sql import Row, SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window as W\n",
    "# from pyspark.sql.functions import (col, collect_list, concat,\n",
    "#                                    create_map, lit, map_concat,\n",
    "#                                    map_from_arrays, substring_index,\n",
    "#                                    to_json, when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>generate-statistics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa5e1c5c7f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master('local[*]')\n",
    "         .config('spark.driver.memory', '10G')\n",
    "         .appName('generate-statistics')\n",
    "         .getOrCreate())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 500 Row Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8min 12s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "df_500 = spark.read.parquet('../data/df_500/')\n",
    "\n",
    "# This method will create a huge plan \n",
    "#  if there is a large number of columns\n",
    "# An RDD based method might be better \n",
    "#  in this case\n",
    "#\n",
    "# def wide_to_long_sql(df):\n",
    "#     df = df_500\n",
    "#     df_columns = df.columns\n",
    "#     df_columns_iterator = (\n",
    "#         (df.select(\n",
    "#             lit(column).alias('col_name'),\n",
    "#             col(column).alias('col_value')))\n",
    "#         for column in df_columns)\n",
    "#     long_df = reduce(lambda x,y: x.union(y), df_columns_iterator)\n",
    "#     return long_df\n",
    "\n",
    "def wide_to_long_dfs(df):\n",
    "    def _emit_column_records(row):\n",
    "        for col, val in row.asDict().items():\n",
    "            yield Row(col_name=col, col_value=val)\n",
    "    \n",
    "    df_dtypes = df.dtypes\n",
    "    \n",
    "    dtype_dfs = {}\n",
    "    \n",
    "    for dtype in set(d for c, d in df_dtypes):\n",
    "        dtype_cols = [c for c, d in df_dtypes\n",
    "                      if d == dtype]\n",
    "        \n",
    "        dtype_df = (\n",
    "            spark.createDataFrame(\n",
    "                df.select(*dtype_cols)\n",
    "                .rdd.flatMap(_emit_column_records)))\n",
    "        \n",
    "        dtype_dfs[dtype] = dtype_df\n",
    "    \n",
    "    return dtype_dfs\n",
    "\n",
    "long_dfs_by_datatype = wide_to_long_dfs(df_500)\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    base_metrics_df = (\n",
    "        df.groupBy('col_name')\n",
    "        .agg(F.count('col_value').alias('count'),\n",
    "             F.mean('col_value').alias('mean'),\n",
    "             F.stddev('col_value').alias('stddev'),\n",
    "             F.min('col_value').alias('min'),\n",
    "             F.max('col_value').alias('max')))\n",
    "\n",
    "    percentiles_df = (\n",
    "        df\n",
    "        .withColumn('percent_rank', (F.percent_rank()\n",
    "                                     .over(\n",
    "                                         W.partitionBy('col_name')\n",
    "                                         .orderBy('col_value'))))\n",
    "        .withColumn('ge_p1', (F.when(F.col('percent_rank') <= 0.01,\n",
    "                                     0.01 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p5', (F.when(F.col('percent_rank') <= 0.05,\n",
    "                                     0.05 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p25', (F.when(F.col('percent_rank') <= 0.25,\n",
    "                                      0.25 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p50', (F.when(F.col('percent_rank') <= 0.5,\n",
    "                                      0.5 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p75', (F.when(F.col('percent_rank') <= 0.75,\n",
    "                                      0.75 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p95', (F.when(F.col('percent_rank') <= 0.95,\n",
    "                                      0.95 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p99', (F.when(F.col('percent_rank') <= 0.99,\n",
    "                                      0.99 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('min_ge_p1', (F.min(F.col('ge_p1'))\n",
    "                                  .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p5', (F.min(F.col('ge_p5'))\n",
    "                                  .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p25', (F.min(F.col('ge_p25'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p50', (F.min(F.col('ge_p50'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p75', (F.min(F.col('ge_p75'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p95', (F.min(F.col('ge_p95'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p99', (F.min(F.col('ge_p99'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('p1', (F.when(F.col('ge_p1') == F.col('min_ge_p1'),\n",
    "                                  F.col('col_value'))\n",
    "                           .otherwise(F.lit(None))))\n",
    "        .withColumn('p5', (F.when(F.col('ge_p5') == F.col('min_ge_p5'),\n",
    "                                  F.col('col_value'))\n",
    "                           .otherwise(F.lit(None))))\n",
    "        .withColumn('p25', (F.when(F.col('ge_p25') == F.col('min_ge_p25'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p50', (F.when(F.col('ge_p50') == F.col('min_ge_p50'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p75', (F.when(F.col('ge_p75') == F.col('min_ge_p75'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p95', (F.when(F.col('ge_p95') == F.col('min_ge_p95'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p99', (F.when(F.col('ge_p99') == F.col('min_ge_p99'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .groupBy('col_name')\n",
    "        .agg(F.min('p1').alias('p1'),\n",
    "             F.min('p5').alias('p5'),\n",
    "             F.min('p25').alias('p25'),\n",
    "             F.min('p50').alias('p50'),\n",
    "             F.min('p75').alias('p75'),\n",
    "             F.min('p95').alias('p95'),\n",
    "             F.min('p99').alias('p99'))\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        base_metrics_df\n",
    "        .join(other=percentiles_df,\n",
    "              on='col_name',\n",
    "              how='inner')\n",
    "        .withColumnRenamed('col_name', 'name'))\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "result_list = []\n",
    "for df in long_dfs_by_datatype.values():\n",
    "    res_df = calculate_metrics(df)\n",
    "    result_list.extend([row.asDict() for row\n",
    "                        in res_df.toLocalIterator()])\n",
    "\n",
    "with open('../data/metrics_for_500_custom.json', 'w') as f:\n",
    "    json.dump(result_list, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the Distributed Stage in the Summary Task:** Input Size / Records\t28.5 MB / 25000\n",
    "\n",
    "**Timing Information for whole job:** 8min 12s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3000 Row Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50min 19s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "df_3000 = spark.read.parquet('../data/df_3000/')\n",
    "\n",
    "\n",
    "def wide_to_long_dfs(df):\n",
    "    def _emit_column_records(row):\n",
    "        for col, val in row.asDict().items():\n",
    "            yield Row(col_name=col, col_value=val)\n",
    "    \n",
    "    df_dtypes = df.dtypes\n",
    "    \n",
    "    dtype_dfs = {}\n",
    "    \n",
    "    for dtype in set(d for c, d in df_dtypes):\n",
    "        dtype_cols = [c for c, d in df_dtypes\n",
    "                      if d == dtype]\n",
    "        \n",
    "        dtype_df = (\n",
    "            spark.createDataFrame(\n",
    "                df.select(*dtype_cols)\n",
    "                .rdd.flatMap(_emit_column_records)))\n",
    "        \n",
    "        dtype_dfs[dtype] = dtype_df\n",
    "    \n",
    "    return dtype_dfs\n",
    "\n",
    "long_dfs_by_datatype = wide_to_long_dfs(df_3000)\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    base_metrics_df = (\n",
    "        df.groupBy('col_name')\n",
    "        .agg(F.count('col_value').alias('count'),\n",
    "             F.mean('col_value').alias('mean'),\n",
    "             F.stddev('col_value').alias('stddev'),\n",
    "             F.min('col_value').alias('min'),\n",
    "             F.max('col_value').alias('max')))\n",
    "\n",
    "    percentiles_df = (\n",
    "        df\n",
    "        .withColumn('percent_rank', (F.percent_rank()\n",
    "                                     .over(\n",
    "                                         W.partitionBy('col_name')\n",
    "                                         .orderBy('col_value'))))\n",
    "        .withColumn('ge_p1', (F.when(F.col('percent_rank') <= 0.01,\n",
    "                                     0.01 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p5', (F.when(F.col('percent_rank') <= 0.05,\n",
    "                                     0.05 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p25', (F.when(F.col('percent_rank') <= 0.25,\n",
    "                                      0.25 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p50', (F.when(F.col('percent_rank') <= 0.5,\n",
    "                                      0.5 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p75', (F.when(F.col('percent_rank') <= 0.75,\n",
    "                                      0.75 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p95', (F.when(F.col('percent_rank') <= 0.95,\n",
    "                                      0.95 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p99', (F.when(F.col('percent_rank') <= 0.99,\n",
    "                                      0.99 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('min_ge_p1', (F.min(F.col('ge_p1'))\n",
    "                                  .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p5', (F.min(F.col('ge_p5'))\n",
    "                                  .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p25', (F.min(F.col('ge_p25'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p50', (F.min(F.col('ge_p50'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p75', (F.min(F.col('ge_p75'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p95', (F.min(F.col('ge_p95'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p99', (F.min(F.col('ge_p99'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('p1', (F.when(F.col('ge_p1') == F.col('min_ge_p1'),\n",
    "                                  F.col('col_value'))\n",
    "                           .otherwise(F.lit(None))))\n",
    "        .withColumn('p5', (F.when(F.col('ge_p5') == F.col('min_ge_p5'),\n",
    "                                  F.col('col_value'))\n",
    "                           .otherwise(F.lit(None))))\n",
    "        .withColumn('p25', (F.when(F.col('ge_p25') == F.col('min_ge_p25'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p50', (F.when(F.col('ge_p50') == F.col('min_ge_p50'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p75', (F.when(F.col('ge_p75') == F.col('min_ge_p75'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p95', (F.when(F.col('ge_p95') == F.col('min_ge_p95'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p99', (F.when(F.col('ge_p99') == F.col('min_ge_p99'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .groupBy('col_name')\n",
    "        .agg(F.min('p1').alias('p1'),\n",
    "             F.min('p5').alias('p5'),\n",
    "             F.min('p25').alias('p25'),\n",
    "             F.min('p50').alias('p50'),\n",
    "             F.min('p75').alias('p75'),\n",
    "             F.min('p95').alias('p95'),\n",
    "             F.min('p99').alias('p99'))\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        base_metrics_df\n",
    "        .join(other=percentiles_df,\n",
    "              on='col_name',\n",
    "              how='inner')\n",
    "        .withColumnRenamed('col_name', 'name'))\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "result_list = []\n",
    "for df in long_dfs_by_datatype.values():\n",
    "    res_df = calculate_metrics(df)\n",
    "    result_list.extend([row.asDict() for row\n",
    "                        in res_df.toLocalIterator()])\n",
    "\n",
    "with open('../data/metrics_for_3000_custom.json', 'w') as f:\n",
    "    json.dump(result_list, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the Distributed Stage in the Summary Task:** Input Size / Records\t137.1 MB / 11182\n",
    "\n",
    "**Timing Information for whole job:** 50min 19s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7min 45s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "df_500_of_3000 = spark.read.parquet('../data/df_3000/')\n",
    "df_500_of_3000 = (df_500_of_3000\n",
    "                  .select(*[col for col\n",
    "                            in df_500_of_3000.columns\n",
    "                            if col.endswith('_1')]))\n",
    "\n",
    "def wide_to_long_dfs(df):\n",
    "    def _emit_column_records(row):\n",
    "        for col, val in row.asDict().items():\n",
    "            yield Row(col_name=col, col_value=val)\n",
    "    \n",
    "    df_dtypes = df.dtypes\n",
    "    \n",
    "    dtype_dfs = {}\n",
    "    \n",
    "    for dtype in set(d for c, d in df_dtypes):\n",
    "        dtype_cols = [c for c, d in df_dtypes\n",
    "                      if d == dtype]\n",
    "        \n",
    "        dtype_df = (\n",
    "            spark.createDataFrame(\n",
    "                df.select(*dtype_cols)\n",
    "                .rdd.flatMap(_emit_column_records)))\n",
    "        \n",
    "        dtype_dfs[dtype] = dtype_df\n",
    "    \n",
    "    return dtype_dfs\n",
    "\n",
    "long_dfs_by_datatype = wide_to_long_dfs(df_500_of_3000)\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    base_metrics_df = (\n",
    "        df.groupBy('col_name')\n",
    "        .agg(F.count('col_value').alias('count'),\n",
    "             F.mean('col_value').alias('mean'),\n",
    "             F.stddev('col_value').alias('stddev'),\n",
    "             F.min('col_value').alias('min'),\n",
    "             F.max('col_value').alias('max')))\n",
    "\n",
    "    percentiles_df = (\n",
    "        df\n",
    "        .withColumn('percent_rank', (F.percent_rank()\n",
    "                                     .over(\n",
    "                                         W.partitionBy('col_name')\n",
    "                                         .orderBy('col_value'))))\n",
    "        .withColumn('ge_p1', (F.when(F.col('percent_rank') <= 0.01,\n",
    "                                     0.01 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p5', (F.when(F.col('percent_rank') <= 0.05,\n",
    "                                     0.05 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p25', (F.when(F.col('percent_rank') <= 0.25,\n",
    "                                      0.25 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p50', (F.when(F.col('percent_rank') <= 0.5,\n",
    "                                      0.5 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p75', (F.when(F.col('percent_rank') <= 0.75,\n",
    "                                      0.75 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p95', (F.when(F.col('percent_rank') <= 0.95,\n",
    "                                      0.95 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('ge_p99', (F.when(F.col('percent_rank') <= 0.99,\n",
    "                                      0.99 - F.col('percent_rank'))\n",
    "                              .otherwise(F.lit(None))))\n",
    "        .withColumn('min_ge_p1', (F.min(F.col('ge_p1'))\n",
    "                                  .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p5', (F.min(F.col('ge_p5'))\n",
    "                                  .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p25', (F.min(F.col('ge_p25'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p50', (F.min(F.col('ge_p50'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p75', (F.min(F.col('ge_p75'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p95', (F.min(F.col('ge_p95'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('min_ge_p99', (F.min(F.col('ge_p99'))\n",
    "                                   .over(W.partitionBy('col_name'))))\n",
    "        .withColumn('p1', (F.when(F.col('ge_p1') == F.col('min_ge_p1'),\n",
    "                                  F.col('col_value'))\n",
    "                           .otherwise(F.lit(None))))\n",
    "        .withColumn('p5', (F.when(F.col('ge_p5') == F.col('min_ge_p5'),\n",
    "                                  F.col('col_value'))\n",
    "                           .otherwise(F.lit(None))))\n",
    "        .withColumn('p25', (F.when(F.col('ge_p25') == F.col('min_ge_p25'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p50', (F.when(F.col('ge_p50') == F.col('min_ge_p50'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p75', (F.when(F.col('ge_p75') == F.col('min_ge_p75'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p95', (F.when(F.col('ge_p95') == F.col('min_ge_p95'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .withColumn('p99', (F.when(F.col('ge_p99') == F.col('min_ge_p99'),\n",
    "                                   F.col('col_value'))\n",
    "                            .otherwise(F.lit(None))))\n",
    "        .groupBy('col_name')\n",
    "        .agg(F.min('p1').alias('p1'),\n",
    "             F.min('p5').alias('p5'),\n",
    "             F.min('p25').alias('p25'),\n",
    "             F.min('p50').alias('p50'),\n",
    "             F.min('p75').alias('p75'),\n",
    "             F.min('p95').alias('p95'),\n",
    "             F.min('p99').alias('p99'))\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        base_metrics_df\n",
    "        .join(other=percentiles_df,\n",
    "              on='col_name',\n",
    "              how='inner')\n",
    "        .withColumnRenamed('col_name', 'name'))\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "result_list = []\n",
    "for df in long_dfs_by_datatype.values():\n",
    "    res_df = calculate_metrics(df)\n",
    "    result_list.extend([row.asDict() for row\n",
    "                        in res_df.toLocalIterator()])\n",
    "\n",
    "with open('../data/metrics_for_500_of_3000_custom.json', 'w') as f:\n",
    "    json.dump(result_list, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the Distributed Stage in the Summary Task:** Input Size / Records\t22.5 MB / 11182\n",
    "\n",
    "**Timing Information for whole job:** 7min 45s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, the summaries are getting calculated in time linear w.r.t. the number of input columns. And hence, meets with the expectation.\n",
    "\n",
    "The choice to convert from wide format to long format was done to logically partition the metrics calculation task for each column. \n",
    "\n",
    "There is a further scope for improvement by combining the two separate steps being used to calculate the base metrics and the percentiles. This can potentially bring the execution time down by half. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
